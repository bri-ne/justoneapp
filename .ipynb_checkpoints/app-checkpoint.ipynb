{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c111e0",
   "metadata": {},
   "source": [
    "# This Panel will call comments from  r/all, r/news, and r/politics and predict if robot or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f449a1d",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f23fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789a63e",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6401b",
   "metadata": {},
   "source": [
    "### Fn: Get the posts, their comments, and clean them up (Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175bd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Get The Posts\n",
    "def getpostidsHOT(sub):  \n",
    "    \"\"\"\n",
    "    This fn gets the top 10 posts for a subreddit sorted by hot\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    title = []\n",
    "    subid = []\n",
    "    for submission in subreddit.hot(limit=10):\n",
    "        tit = submission.title\n",
    "        title.append(tit)\n",
    "        ids = submission.id\n",
    "        subid.append(ids)\n",
    "    df = pd.DataFrame(list(zip(title, subid)), columns=[\"title\", \"id\"])\n",
    "    print(\"got post ids\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec655ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Then Get The Comments\n",
    "def getcomments(id, postnumber):\n",
    "    \"\"\"\n",
    "    This function gets comments and a few other field. It formats in a way ready for the\n",
    "    cleandata fn.\n",
    "    \"\"\"\n",
    "    submission = reddit.submission(id=id)\n",
    "    comid = []\n",
    "    authoru = []\n",
    "    mods = []\n",
    "    created = []\n",
    "    upratio = []\n",
    "    body = []\n",
    "    submission.comments.replace_more(limit=5)  \n",
    "    for comment in submission.comments.list():\n",
    "        ids = comment.id\n",
    "        author = comment.author\n",
    "        mod = comment.distinguished\n",
    "        createds = comment.created_utc\n",
    "        score = comment.score\n",
    "        bodys = comment.body\n",
    "        \n",
    "        comid.append(ids)\n",
    "        authoru.append(author)\n",
    "        mods.append(mod)\n",
    "        created.append(createds)\n",
    "        upratio.append(score)\n",
    "        body.append(bodys)\n",
    "    df = pd.DataFrame(list(zip(comid, authoru, mods, created, upratio, body)), \n",
    "                              columns=[\"comid\", \"authoru\", \"mods\", 'created', 'upratio', 'body'])\n",
    "    postnumber = df\n",
    "    print(\"got comments\")\n",
    "    return postnumber\n",
    "\n",
    "###################################################################\n",
    "## Then get a little bit more data that be useful later\n",
    "def cleandata (df):\n",
    "    \"\"\"\n",
    "    This function extracts usernames from the author feature even where the name is deleted.\n",
    "    It also cleans up the time to make it more useable.\n",
    "    \"\"\"\n",
    "    created_date = []\n",
    "    names = []\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            uname = df.loc[i][1].name\n",
    "            names.append(uname)\n",
    "        except:    \n",
    "            uname = 'deleted'  \n",
    "            names.append(uname)\n",
    "    df[\"username\"] = names\n",
    "    \n",
    "    for row in df[\"created\"]:\n",
    "        date = datetime.datetime.fromtimestamp(row)\n",
    "        created_date.append(date)\n",
    "\n",
    "    print(\"done\")\n",
    "    df[\"created_date\"] = created_date\n",
    "\n",
    "\n",
    "    ## Dealing with TIME~~\n",
    "    df[\"created_date\"] = pd.to_datetime(df[\"created_date\"])\n",
    "    dattime_strings = df[\"created_date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    df[\"created_date2\"] = dattime_strings\n",
    "    #complaints.head()\n",
    "\n",
    "    df[\"created_date2\"]=df[\"created_date2\"].apply(lambda x:datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    time_strings = df[\"created_date\"].dt.strftime(\"%H:%M:%S\")\n",
    "    date_strings = df[\"created_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df[\"date\"] = date_strings\n",
    "    df[\"time\"] = time_strings\n",
    "    print(\"claned time and got users\")\n",
    "    return df\n",
    "\n",
    "#######################################################################\n",
    "## Then use Eamon Flemming's Cleaning Strategy\n",
    "\n",
    "#Import Regex\n",
    "import re\n",
    "\n",
    "#This function selects any consecutive combination of \\r's and \\n's in a bloc of text, \n",
    "#and replaces that selection with a single space.\n",
    "def replace_linebreaks_w_space(x):\n",
    "    return re.sub('([\\r\\n]+)',' ',x)\n",
    "\n",
    "#This function selects any stretch of two or more consecutive spaces in a bloc of text,\n",
    "#and replaces that selection with a single space.\n",
    "def replace_multispace_w_space(x):\n",
    "    return re.sub('([ ]{2,})',' ',x)\n",
    "\n",
    "\"\"\"\n",
    "I created a fn to tie all  eamon's fn together:\n",
    "\"\"\"\n",
    "def use_eamons(df, sub, body = \"body\"):\n",
    "    #Here we take every comment and apply the two functions to it.\n",
    "    df[body] = df[body].map(replace_linebreaks_w_space)\n",
    "    df[body] = df[body].map(replace_multispace_w_space)\n",
    "    \n",
    "    #Strip away any spaces at the beginning or end of each comment, splits the comment into a list of words, \n",
    "    #and returns the length of that list (i.e.; the number of words in the comment)\n",
    "    df['word_length'] = df[body].map(lambda x: len(x.strip().split(' ')))\n",
    "    df[\"subreddit\"] = sub\n",
    "    \n",
    "    \"\"\"\n",
    "    The last thing we're going to do is remove all comments that are 3 words and shorter, as it's difficult, \n",
    "    and for the most part just unreasonable, to guess anything from comments this short. \n",
    "    We want to focus on accurately predicting comments that have some content.\n",
    "    \"\"\"\n",
    "    comments = df[df['word_length']>=4]\n",
    "    \n",
    "    df_clean = comments\n",
    "    print(\"used eamons\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92793080",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Then combine everything but the post ids into one fn\n",
    "def allsteps(df_name, subname):  \n",
    "    i = 0\n",
    "    df_list = []\n",
    "    for row in df_name[\"id\"]:\n",
    "\n",
    "        print(row)\n",
    "        i += 1\n",
    "        name = f\"row{i}\"\n",
    "        df = getcomments(row, name)\n",
    "        df = cleandata(df)\n",
    "        df_list.append(df)\n",
    "\n",
    "    clean_df_list=[]\n",
    "    for df in df_list:\n",
    "        clean_df = use_eamons(df, subname)\n",
    "        clean_df_list.append(clean_df)\n",
    "\n",
    "    final_df = pd.concat(clean_df_list)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d6821",
   "metadata": {},
   "source": [
    "### Look for Accusations of Bots and Get the Comments Accused "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed3161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Accusation Comments\n",
    "\n",
    "def findbotaccuse(df, body_column):\n",
    "    \"\"\"\n",
    "    This function looks for the word bot, then cleans for mod bots that say -i am a bot-. it assumes everything \n",
    "    that is keeps is an accusation. While that's likely not true, we'll roll with it and see.\n",
    "    \"\"\"\n",
    "    bot = df[df[body_column].str.contains(r\"\\bbot\\b\")]\n",
    "    Bot = df[df[body_column].str.contains(r\"\\bBot\\b\")]\n",
    "    robot = df[df[body_column].str.contains(r\"\\brobot\\b\")]\n",
    "    Robot = df[df[body_column].str.contains(r\"\\bRobot\\b\")]\n",
    "    \n",
    "    bot_accuse = pd.concat([bot, robot, Robot, Bot])\n",
    "    bot_accuse = bot_accuse.reset_index(drop=True)\n",
    "    \n",
    "    #cleaning for -i am a bot-\n",
    "    only_accuse_bot = bot_accuse[~bot_accuse[body_column].str.contains(r\"\\bI am a bot\\b\")]\n",
    "    print(\"found accusations\")\n",
    "    return only_accuse_bot\n",
    "\n",
    "######################################################\n",
    "## Then get accused\n",
    "\n",
    "def getparentid(df, idcol, bodycol):\n",
    "    \"\"\"\n",
    "    This gets the id of the parents that we can use to get the body\n",
    "    \"\"\"\n",
    "\n",
    "    com_id = []\n",
    "    parent = []\n",
    "    com_body = []\n",
    "    for comid in df[idcol]:\n",
    "        comment_id = comid\n",
    "        com_id.append(comment_id)\n",
    "\n",
    "        # instantiating the Comment class\n",
    "        comment = reddit.comment(comment_id)\n",
    "\n",
    "        # fetching the parent_id attribute\n",
    "        parent_id = comment.parent_id \n",
    "        print(parent_id)\n",
    "        # collecting\n",
    "        parent.append(parent_id)\n",
    "    for body in df[bodycol]:\n",
    "        bodyi = body\n",
    "        com_body.append(bodyi)\n",
    "        \n",
    "    print(\"got parent ID\")\n",
    "    return parent, com_id, com_body\n",
    "\n",
    "\n",
    "def getparentbody(parent, com_id, com_body):\n",
    "    \"\"\"\n",
    "    This gets the body of the parent comment and produces the final df with all the \n",
    "    info we need\n",
    "    \"\"\"\n",
    "    parentbody=[]\n",
    " \n",
    "    for ids in parent:\n",
    "        try:\n",
    "            parent_id = ids\n",
    "            # but also running through next step\n",
    "            comment_parent = reddit.comment(parent_id)\n",
    "            parent_body = comment_parent.body\n",
    "\n",
    "\n",
    "            print(\"got parent body\")\n",
    "        except:\n",
    "            parent_body = \" \"\n",
    "            print(\"no body\")\n",
    "        \n",
    "        parentbody.append(parent_body)    \n",
    "    \n",
    "    lil_df = pd.DataFrame(list(zip(com_id, com_body, parent, parentbody)), columns=[\"child_id\", \"child_body\", \"parent_id\", \"parent_body\"])\n",
    "    \n",
    "    return lil_df\n",
    "\n",
    "## putting get accused into one neat fn: \n",
    "\n",
    "def getparentall(df, idcol, bodycol):\n",
    "    \"\"\"\n",
    "    This combines the get parent body and get id. it wasn't working when i had them in one singular fn, but it works\n",
    "    like this\n",
    "    \"\"\"\n",
    "    parent_list, com_id, com_body  = getparentid(df, idcol, bodycol)\n",
    "    final_df = getparentbody(parent_list, com_id, com_body)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49858842",
   "metadata": {},
   "source": [
    "### Last Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ecf6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded pickled model to make predictions\n",
    "def getpredictions(model, Xval, df):\n",
    "    \"\"\"\n",
    "    This function predicts with the defined model on the validation set\n",
    "    \"\"\"\n",
    "    predictions = pd.DataFrame(model.predict_proba(Xval))\n",
    "    predictions['parent_body'] = df['parent_body']\n",
    "    predictions['child_body'] = df['child_body']\n",
    "    predictions['child_id'] = df['child_id']\n",
    "    predictions['parent_id'] = df['parent_id']\n",
    "    predictions['pred'] = model.predict(Xval)\n",
    "    predictions.columns = ['bot', 'human','parent_body','child_body','child_id','parent_id','pred']\n",
    "    predictions = predictions[['child_id','child_body','parent_id','parent_body','pred', 'human','bot']]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34287712",
   "metadata": {},
   "source": [
    "## setup credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd2e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID =  \"4OwAYLyOEKKPQjb-Hgpt6g\"\n",
    "SECRET_TOKEN =  \"-05d1ux9vDdkVt67yPMojiKywTjRsQ\"\n",
    "user_agent =  \"MUSA550KarmaMine1\"\n",
    "password = 'KarmaMine1$$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3bb0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=CLIENT_ID, \\\n",
    "                     client_secret=SECRET_TOKEN, \\\n",
    "                     user_agent=user_agent, \\\n",
    "                     username=user_agent, \\\n",
    "                     password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420bf22",
   "metadata": {},
   "source": [
    "## Call Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4733e1",
   "metadata": {},
   "source": [
    "### r/all (sorted by HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66498e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got post ids\n",
      "rl0rbu\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl2ckv\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkzlrz\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl14fx\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkz9so\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkzlar\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl1ijq\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkz3gg\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkyn0t\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkzdyh\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n"
     ]
    }
   ],
   "source": [
    "rall_df = getpostidsHOT(\"all\")\n",
    "rall_comments = allsteps(rall_df, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fee22",
   "metadata": {},
   "source": [
    "### r/news  (sorted by HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af21c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got post ids\n",
      "rkulbg\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl1y03\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkzeoj\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkqjyx\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rko3lm\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl4atv\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkz5f0\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkp52l\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl3li2\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl1xs5\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n"
     ]
    }
   ],
   "source": [
    "rnews_df = getpostidsHOT(\"news\")\n",
    "rnews_comments = allsteps(rnews_df, \"news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3b590",
   "metadata": {},
   "source": [
    "### r/politics  (sorted by HOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063e6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got post ids\n",
      "rkt3lz\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkzdyh\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkwvl2\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkwyar\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkzxm4\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkn2t0\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkycju\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rknwk8\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rl3gxq\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "rkrlfz\n",
      "got comments\n",
      "done\n",
      "claned time and got users\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n",
      "used eamons\n"
     ]
    }
   ],
   "source": [
    "rpolitics_df = getpostidsHOT(\"politics\")\n",
    "rpolitics_comments = allsteps(rpolitics_df, \"politics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c57811",
   "metadata": {},
   "source": [
    "## Get Accusations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d84e66fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found accusations\n",
      "found accusations\n",
      "found accusations\n"
     ]
    }
   ],
   "source": [
    "## rall\n",
    "rall_accuse = findbotaccuse(rall_comments, \"body\")\n",
    "\n",
    "\n",
    "## rnews\n",
    "rnews_accuse = findbotaccuse(rnews_comments, \"body\")\n",
    "\n",
    "\n",
    "## rpolitics\n",
    "rpolitics_accuse = findbotaccuse(rpolitics_comments, \"body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef7e15",
   "metadata": {},
   "source": [
    "## Getting the Accused (Parent of the Accusation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee2930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_rl2ckv\n",
      "t1_hpdidyr\n",
      "t1_hpd6lfd\n",
      "t1_hpdb1m4\n",
      "t1_hpdjleo\n",
      "t1_hpdqlg7\n",
      "t1_hpdqlg7\n",
      "t1_hpdpqc3\n",
      "got parent ID\n",
      "no body\n",
      "got parent body\n",
      "got parent body\n",
      "got parent body\n",
      "got parent body\n",
      "got parent body\n",
      "got parent body\n",
      "got parent body\n",
      "t1_hpbyjuu\n",
      "got parent ID\n",
      "got parent body\n",
      "t1_hpbikxs\n",
      "t3_rkycju\n",
      "t1_hpar46b\n",
      "t1_hpd144m\n",
      "got parent ID\n",
      "got parent body\n",
      "no body\n",
      "got parent body\n",
      "got parent body\n"
     ]
    }
   ],
   "source": [
    "## rall\n",
    "rall_parent = getparentall(rall_accuse, \"comid\", \"body\")\n",
    "\n",
    "\n",
    "## rnews\n",
    "rnews_parent = getparentall(rnews_accuse, \"comid\", \"body\")\n",
    "\n",
    "\n",
    "## rpolitics\n",
    "rpolitics_parent = getparentall(rpolitics_accuse, \"comid\", \"body\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0f76a",
   "metadata": {},
   "source": [
    "## Cleaning the Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "472b0a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used eamons\n"
     ]
    }
   ],
   "source": [
    "rall_parent_clean = use_eamons(rall_parent, \"rall\", \"parent_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb4a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used eamons\n"
     ]
    }
   ],
   "source": [
    "rnews_parent_clean = use_eamons(rnews_parent, \"rnews\", \"parent_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fdddac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used eamons\n"
     ]
    }
   ],
   "source": [
    "rpolitics_parent_clean = use_eamons(rpolitics_parent, \"rpolitics\", \"parent_body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830e62e",
   "metadata": {},
   "source": [
    "## Bringing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea6b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a3db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'./data/rallsubsimLogit.joblib.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56247e8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/rallsubsimLogit.joblib.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31364/297842909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\musa-550-fall-2021\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/rallsubsimLogit.joblib.pkl'"
     ]
    }
   ],
   "source": [
    "model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586b83c",
   "metadata": {},
   "source": [
    "## Testing on the Accusations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d81aa9",
   "metadata": {},
   "source": [
    "### r/all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xall = rall_parent_clean['parent_body']\n",
    "rall_predictions = getpredictions(model, Xall, rall_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c74dd3",
   "metadata": {},
   "source": [
    "### r/news  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09efee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnews = rnews_parent_clean['parent_body']\n",
    "news_predictions = getpredictions(model, Xnews, rnews_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d27eb",
   "metadata": {},
   "source": [
    "### r/politics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpolitics = rpolitics_parent_clean['parent_body']\n",
    "politics_predictions = getpredictions(model, Xpolitics, rpolitics_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b1eed",
   "metadata": {},
   "source": [
    "# combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [rall_predictions, news_predictions, politics_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c691a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "accused = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd62e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accused.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ee3ef",
   "metadata": {},
   "source": [
    "## Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fff776",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = [textblob.TextBlob(posts) for posts in accused[\"parent_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d61172",
   "metadata": {},
   "outputs": [],
   "source": [
    "accused['Polarity'] = [blob.sentiment.polarity for blob in blobs]\n",
    "accused['Subjectivity'] = [blob.sentiment.subjectivity for blob in blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "accused.rename(columns={'parent_body': 'accused', 'child_body': 'accuser'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8ca25",
   "metadata": {},
   "source": [
    "## HV Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d98c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BotApp(pm.Parameterized):\n",
    "\n",
    "    def accused_plot(x='human', y='bot', width= 320, color='#4b2362', hover_cols=[\"accuser\", \"accused\"]):\n",
    "        return accused.hvplot.scatter(x, y, c=c, padding=0.1)\n",
    "    \n",
    "    # Create the widgets\n",
    "    x = pn.widgets.Select(value=\"human\", options=columns, name=\"x\")\n",
    "    y = pn.widgets.Select(value=\"bot\", options=columns, name=\"y\")\n",
    "    #color = pn.widgets.ColorPicker(name=\"Color\", value=\"#4b2362\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our app\n",
    "app = BotApp(name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dashboard\n",
    "reactive_dashboard = pn.Row(\n",
    "    pn.Column(\n",
    "        pn.Row(\"Bot Accused Comments\"),\n",
    "        pn.Row(\n",
    "            pn.Column(x, y)), # Title and widgets\n",
    "    pn.bind(accused.hvplot.scatter, x, y, width= 320,  c='#4b2362', hover_cols=[\"accuser\", \"accused\"])), # Main chart\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "reactive_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pn.template.BootstrapTemplate(\n",
    "    title=\"Predicting Bots on Reddit\",\n",
    "    sidebar_width=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fff697",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.main.append(\n",
    "    pn.Row(\n",
    "        pn.Column(\n",
    "            pn.Row(\"Bot Accused Comments\"),\n",
    "            pn.Row(\n",
    "                pn.Column(x, y)), # Title and widgets\n",
    "        pn.bind(accused.hvplot.scatter, x, y, width= 320,  c='#4b2362', hover_cols=[\"accuser\", \"accused\"])), # Main chart\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97043efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b21b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
